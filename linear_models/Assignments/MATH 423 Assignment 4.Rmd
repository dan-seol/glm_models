---
title: "Assignment 4"
author: "YUNHEUM DAN SEOL"
date: '2017-12-06'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Question 1
1.TestScores.csv contains data on standardized math test scores of 45 students from three Faculties in a university

##(a)

```{r}
library(readr)
Scores <- read_csv('http://www.math.mcgill.ca/yyang/regression/data/TestScores.csv')
str(Scores)
head(Scores)
Scores$Faculty <- as.factor(Scores$Faculty)
str(Scores)
fit.Scores.1 <- lm(Score~I(Faculty), data =Scores)
summary(fit.Scores.1)
anova(fit.Scores.1) 
```
We can conclude that there is a significant difference of scores between Faculties looking at the global fit from our F-test; moreover, we can further test between what specific faculties we can find evidence for the difference in scores and check whether our model actually holds by checking the residual plot. We do this in part (b). 

##(b)
```{r}
library(lsmeans)
fit.Scores.2 <- lm(Score ~-1+I(Faculty), data = Scores)
summary(fit.Scores.2)

#Acquiring means through lsmeans()
lsmeans(fit.Scores.2, ~I(Faculty))

#Residual plot
residual.data<-data.frame(Residuals=residuals(fit.Scores.1),Faculty =Scores$Faculty) 
par(mar=c(4,4,1,2))
stripchart(Residuals ~ Faculty,data = residual.data,pch=3,vertical=T,ylim=range(-20,15),
           xlab='Faculty') ;abline(h=0,lty=2)
 
```

The estimated mean score for faculty 1 would be 35.8, for faculty 2 would be $35.8000+0.0667=35.8667$, and for faculty 3 it would be $35.8+12.4=48.2$ with respective standard error 1.595894 for all faculties. 
A visual inspection of the residual plot suggests that the group variance for Faculty 3 might be smaller than that of others.

#Question 2
```{r}
#Reading in date on the noise emission level of 36 cars
Filter <- read_csv("http://www.math.mcgill.ca/yyang/regression/data/Filter.csv")
str(Filter)
```
```{r}
fmodel1 <-lm(noise~1,data=Filter);summary(fmodel1)
anova(fmodel1)
#Adding the sum function to get the numeric data
SSres1<-sum(anova(fmodel1)[2])
```
```{r}
fmodel2 <-lm(noise~I(carsize),data=Filter);summary(fmodel2)
anova(fmodel2)
SSres2<-sum(anova(fmodel2)[2,2])
```
```{r}
fmodel3 <-lm(noise~I(type),data=Filter);summary(fmodel3)
anova(fmodel3)
SSres3<-sum(anova(fmodel3)[2,2])
```
```{r}
fmodel4 <-lm(noise~I(carsize)+I(type),data=Filter);summary(fmodel4)
anova(fmodel4)
SSres4<-sum(anova(fmodel4)[3,2])

```
```{r}
fmodel5 <-lm(noise~I(carsize)*I(type),data=Filter);summary(fmodel5)
anova(fmodel5)
SSres5<-sum(anova(fmodel5)[4,2])
```
```{r}
t <- c("1", "1+X1", "1+X2", "1+X1+X2", "1+X1+X2+X1:X2")
v <- c(SSres1, SSres2, SSres3, SSres4, SSres5)
w <- c("Intercept", "Main Effect of Car Size", 
       "Main Effect of Filter Type", "Main Effects only",
       "Main Effects plus interactions")
u <- c("lm(noise~1",
       "lm(noise~I(carsize))",
       "lm(noise~I(type))",
       "lm(noise~I(carsize)+I(type))",
       "lm(noise~I(carsize)*I(type))")
mtx <- cbind(t,w,round(v,3),u)
colnames(mtx) <- c("Model","Sum of Sq", "Description", "R")
mtx <- data.frame(mtx)
#table generated by kable(mtx, format = "latex") below
```




\begin{table}[ht]
\centering
\begin{tabular}{r|l|l|l|l}
  \hline
 & Model & Sum.of.Sq & Description & R \\ 
  \hline
1 & 1 & Intercept & 29874.306 & lm(noise\~{}1) \\ 
  2 & 1+X1 & Main Effect of Car Size & 3822.917 & lm(noise\~{}I(carsize)) \\ 
  3 & 1+X2 & Main Effect of Filter Type & 28818.056 & lm(noise\~{}I(type)) \\ 
  4 & 1+X1+X2 & Main Effects only & 2766.667 & lm(noise\~{}I(carsize)+I(type)) \\ 
  5 & 1+X1+X2+X1:X2 & Main Effects plus interactions & 1962.5 & lm(noise\~{}I(carsize)*I(type)) \\ 
   \hline
\end{tabular}
\end{table}
##part (b)

```{r}
anova(fmodel2,fmodel5)
sum(anova(fmodel2, fmodel5)[2,3])
df1 <- sum(anova(fmodel2, fmodel5)[2,3]);df2<-sum(anova(fmodel2, fmodel5)[2,1])
c(df1, df2)
fstat <- sum(anova(fmodel2, fmodel5)[2,5]);fstat
crit.value <- qf(0.95,df1,df2);crit.value
pvalue<- 1-pf(fstat,df1,df2);pvalue
```
at $$\alpha = 0.05$$, we get the F critical value $2.922277$, since our F statistic $9,4798$ is larger than the critical value, we would reject our null hypothesis in the test 
$$H_0= \mathrm{E}[Y_i|\mathbf{x_i}]=1+X_1 \\H_a=\mathrm{E}[Y_i|\mathbf{x_i}]=1+X_1+X_2+X_1X_2$$
Where X1 = (the main effect of the size of the car to noise) and X2 = (the main effect of the filter type) and X1X2 being the interaction term

#Question 3
```{r}
#Reading in the information on patient satisfaction for 25 patients 
#having undergone treatment at a hospital for the same condition
PS <- read.csv("http://www.math.mcgill.ca/yyang/regression/data/PatSat.csv", header=TRUE)
head(PS)
str(PS)

#Renaming predictors and responses to make it simple
y_PS <- PS$Satisfaction
x1_PS <- PS$Surgery
x2_PS <- PS$Age
x3_PS <- PS$Severity
x4_PS <- PS$Anxiety
#Pairwise scatterplots
pairs(PS, pch=3)
#Fitting models
fit0_PS<-lm(y_PS~1)
fit1_PS<-lm(y_PS~I(x1_PS))
fit2_PS<-lm(y_PS~x2_PS)
fit3_PS<-lm(y_PS~x3_PS)
fit4_PS<-lm(y_PS~x4_PS)
fit12_PS<-lm(y_PS~I(x1_PS)+x2_PS)
fit13_PS<-lm(y_PS~I(x1_PS)+x3_PS)
fit14_PS<-lm(y_PS~I(x1_PS)+x4_PS)
fit23_PS<-lm(y_PS~x2_PS+x3_PS)
fit24_PS<-lm(y_PS~x2_PS+x4_PS)
fit34_PS<-lm(y_PS~x3_PS+x4_PS)
fit12i_PS<-lm(y_PS~I(x1_PS)*x2_PS)
fit13i_PS<-lm(y_PS~I(x1_PS)*x3_PS)
fit14i_PS<-lm(y_PS~I(x1_PS)*x4_PS)
fit23i_PS<-lm(y_PS~x2_PS*x3_PS)
fit24i_PS<-lm(y_PS~x2_PS*x4_PS)
fit34i_PS<-lm(y_PS~x3_PS*x4_PS)
fit123_PS<-lm(y_PS~I(x1_PS)+x2_PS+x3_PS)
fit124_PS<-lm(y_PS~I(x1_PS)+x2_PS+x4_PS)
fit134_PS<-lm(y_PS~I(x1_PS)+x3_PS+x4_PS)
fit234_PS<-lm(y_PS~x2_PS+x3_PS+x3_PS)
fit123i_PS<-lm(y_PS~I(x1_PS)*x2_PS*x3_PS)
fit124i_PS<-lm(y_PS~I(x1_PS)*x2_PS*x4_PS)
fit134i_PS<-lm(y_PS~I(x1_PS)*x3_PS*x4_PS)
fit234i_PS<-lm(y_PS~x2_PS*x3_PS*x3_PS)
fit1234_PS<- lm(y_PS~I(x1_PS)+x2_PS+x3_PS+x4_PS)
fit1234i_PS<- lm(y_PS~I(x1_PS)*x2_PS*x3_PS*x4_PS)
```
```{r}
PS_numeric<-PS;PS_numeric$Surgery<-as.numeric(PS_numeric$Surgery)-1 
cor(PS_numeric)
```
It appears that there are some strong correlations among the predictors.
Now, let's fit a model with surgery factor predictor only:
```{r}
summary(fit1_PS)
```
It doesn't seem like as if having surgery significantly affected the patient satisfaction.
```{r}
#We begin the best model identification 
#by examining the additive model that fits all predictors as main effects:

summary(fit1234_PS)

```
```{r}
drop1(fit1234_PS, test='F')
```
It seems that we can drop X1(Surgery) and X4(Anxiety) from the model.
```{r}
#Deleting Anxiety as a predictor
fita_PS <- update(fit1234_PS, ~.-I(x1_PS)-x4_PS)
anova(fita_PS, fit1234_PS, test='F')
#
summary(fita_PS)
```
We can see that we fail to reject the null hypothesis that anxiety is a significant predictor for patient satisfaction. Let's see what roles the interactions might play.
```{r}
fitb_PS<-update(fita_PS, ~.+I(x1_PS)*x2_PS*x4_PS)
fitc_PS<-update(fitb_PS, ~.-I(x1_PS):x2_PS:x4_PS)
anova(fita_PS,fitc_PS,fitb_PS,test='F')
```
It seems that our model with Age and Severity only is still a better model than the others.
```{r}
fitd_PS<- update(fita_PS, ~.+x2_PS:x3_PS)
fite_PS<- update(fitd_PS, ~.+I(x1_PS))
anova(fita_PS, fitd_PS, fite_PS, test='F')
```
Our model with Age and Severity only still remains to be a better model than the others.

Let's try some automated methods
###methods with step command
```{r}
fit.stepa_PS <-step(fit1234i_PS, k=2, trace=0)
print(summary(fit.stepa_PS), concise=T)
```
```{r}
fit.stepb_PS <- update(fit.stepa_PS, ~.-I(x1_PS):x2_PS:x4_PS-x2_PS:x3_PS:x4_PS)
anova(fit.stepb_PS, fit.stepa_PS)
```
```{r}
fit.stepc_PS <- step(lm(y_PS ~(Surgery+x2_PS+x3_PS+x4_PS)^2, data=PS), k=2, trace=0)
summary(fit.stepc_PS)
```
This seems to be  a relatively better model that includes the effect of surgery, but could we come up with a better way to compare models?

###custom methods to compare all the measures : Used a method that was shown in one of the notes ("Factor Predictors - Examples")
```{r}
bigs.hat <- summary(fit1234i_PS)$sigma
criteria.eval <- function(fit.obj,nv,bigsig.hat){
  cvec <- rep(0,5)
  SSRes <- sum(residuals(fit.obj)^2)
  p <- length(coef(fit.obj))
  
  #R Squared
  cvec[1] <- summary(fit.obj)$r.squared
  #Adjusted R Squared
  cvec[2] <- summary(fit.obj)$adj.r.squared
  #Cp
  cvec[3] <- SSRes/bigsig.hat^2-nv+2*p
  #AIC in R computes
  #n*log(sum(residuals(fit.obj)^2)/n)+2*(length(coef(fit.obj)+1)+n*log(2*pi)+n)
  cvec[4] <- AIC(fit.obj)
  #BIC in R computes
  #n*log(sum(residuals(fit.obj)^2)/n)+2*length(coef(fit.obj))+1)+n*log(2*pi)+n
  cvec[5] <- BIC(fit.obj)
  return(cvec)}
```





```{r}
cvals <- matrix(0, nrow=28, ncol=5)

cvals[1, ] <-  criteria.eval(fit0_PS, 25, bigs.hat)
cvals[2, ] <-  criteria.eval(fit1_PS, 25, bigs.hat) 
cvals[3, ] <-  criteria.eval(fit2_PS, 25, bigs.hat) 
cvals[4, ] <-  criteria.eval(fit3_PS, 25, bigs.hat) 
cvals[5, ] <-  criteria.eval(fit4_PS, 25, bigs.hat) 
cvals[6, ] <-  criteria.eval(fit12_PS, 25, bigs.hat) 
cvals[7, ] <-  criteria.eval(fit13_PS, 25, bigs.hat)
cvals[8, ] <-  criteria.eval(fit14_PS, 25, bigs.hat) 
cvals[9, ] <-  criteria.eval(fit23_PS, 25, bigs.hat) 
cvals[10, ] <-  criteria.eval(fit24_PS, 25, bigs.hat)
cvals[11, ] <-  criteria.eval(fit34_PS, 25, bigs.hat) 
cvals[12, ] <-  criteria.eval(fit12i_PS, 25, bigs.hat) 
cvals[13, ] <-  criteria.eval(fit13i_PS, 25, bigs.hat) 
cvals[14, ] <-  criteria.eval(fit14i_PS, 25, bigs.hat) 
cvals[15, ] <-  criteria.eval(fit23i_PS, 25, bigs.hat) 
cvals[16, ] <-  criteria.eval(fit24i_PS, 25, bigs.hat)
cvals[17, ] <-  criteria.eval(fit34i_PS, 25, bigs.hat) 
cvals[18, ] <-  criteria.eval(fit123_PS, 25, bigs.hat) 
cvals[19, ] <-  criteria.eval(fit124_PS, 25, bigs.hat) 
cvals[20, ] <-  criteria.eval(fit134_PS, 25, bigs.hat) 
cvals[21, ] <-  criteria.eval(fit234_PS, 25, bigs.hat) 
cvals[22, ] <-  criteria.eval(fit123i_PS, 25, bigs.hat) 
cvals[23, ] <-  criteria.eval(fit124i_PS, 25, bigs.hat) 
cvals[24, ] <-  criteria.eval(fit134i_PS, 25, bigs.hat) 
cvals[25, ] <-  criteria.eval(fit234i_PS, 25, bigs.hat) 
cvals[26, ] <-  criteria.eval(fit1234_PS, 25, bigs.hat) 
cvals[27, ] <-  criteria.eval(fit1234i_PS, 25, bigs.hat) 
cvals[28,]  <- criteria.eval(fit.stepc_PS, 25, bigs.hat)

Criteria <- data.frame(cvals)

names(Criteria) <- c('Rsq', 'Adj.Rsq', 'Cp', 'AIC', 'BIC')
rownames(Criteria) <- c('Intercept',
                        'Surgery',
                        'Age',
                        'Severity',
                        'Anxiety',
                        'Surgery+Age',
                        'Surgery+Severity',
                        'Surgery+Anxiety',
                        'Age+Severity',
                        'Age+Anxiety',
                        'Severity+Anxiety',
                        'Surgery*Age',
                        'Surgery*Severity',
                        'Surgery*Anxiety',
                        'Age*Severity',
                        'Age*Anxiety',
                        'Severity*Anxiety',
                        'Surgery+Age+Severity',
                        'Surgery+Age+Anxiety',
                        'Surgery+Severity+Anxiety',
                        'Age+Severity+Anxiety',
                        'Surgery*Age*Severity',
                        'Surgery*Age*Anxiety',
                        'Surgery*Severity*Anxiety',
                        'Age*Severity*Anxiety',
                        'Surgery+Age+Severity+Anxiety',
                        'Surgery*Age*Severity*Anxiety', 
                        'Step-selected model')
round(Criteria, 4)
```
```{r}
mtx2 <-rbind(cvals[9,],cvals[15,],cvals[18,],cvals[21,],cvals[25,],cvals[28,])
rownames(mtx2) <- c("Age+Severity","Age*Severity",
                    "Surgery+Age+Severity","Age+Severitty+Anxiety",
                    "Age*Severity*Anxiety","Step-Selected model")
colnames(mtx2) <- c("Rsq","Adj.Rsq","Cp","AIC","BIC")
```
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrr}
  \hline
 & Rsq & Adj.Rsq & Cp & AIC & BIC \\ 
  \hline
Age+Severity & 0.81 & 0.79 & -2.81 & 189.26 & 194.14 \\ 
  Age*Severity & 0.81 & 0.79 & -1.05 & 190.90 & 197.00 \\ 
  Surgery+Age+Severity & 0.81 & 0.78 & -0.99 & 190.99 & 197.08 \\ 
  Age+Severitty+Anxiety & 0.81 & 0.79 & -2.81 & 189.26 & 194.14 \\ 
  Age*Severity*Anxiety & 0.81 & 0.79 & -1.05 & 190.90 & 197.00 \\ 
  Step-Selected model & 0.84 & 0.80 & 0.30 & 190.35 & 198.88 \\ 
   \hline
\end{tabular}
\end{table}

Our conclusion is a rather not-so-neat one: it seems that surgery might have played a mild role in patient satisfaction, but we can't be so sure about it since we only have two models including surgery, and their AIC measures are not the lowest.

Let's further check the effect of predictor Surgery through a visual inspection:
```{r}
PS_no<-PS;PS_no$Surgery<- as.factor('No') 
PS_yes<-PS;PS_yes$Surgery<- as.factor('Yes') 
No_fit<-predict(fit.stepc_PS, newdata=PS_no)
Yes_fit<-predict(fit.stepc_PS, newdata=PS_yes)
par(mar=c(4,4,1,1))
plot(Yes_fit-No_fit,pch=5,xlab='Patients',ylab='Predicted Difference in Satisfaction concerning whether they had a surgery') 
abline(h=0,lty=2)
```

Since significant amount of data plots show there would be positive difference, we can estimate that the predictor Surgery does play a minor role.





